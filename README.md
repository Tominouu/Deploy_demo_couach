# CouachGPT ğŸ¤–

## ğŸ“… Contexte & Objectifs

Ce projet a pour but de **tester, comparer et intÃ©grer diffÃ©rents modÃ¨les LLM** (Large Language Models) exÃ©cutÃ©s **en local** sur serveur sans GPU. Le tout sera liÃ© sur une interface web style "ChatGPT", accessible par tous les utilisateurs connectÃ©s sur le rÃ©seau via cet **IP:** http://172.16.2.81:8294/. L'objectif est de trouver un compromis optimal entre **performances**, **consommation**, et **rÃ©activitÃ©**.

---

## ğŸ”¬ Phase de Recherche & Benchmarks (1â€“4 Juillet)

### ğŸ–¥ï¸ Configuration MatÃ©rielle TestÃ©e

- **RAM** : 16 Ã  24 Go  
- **CPU** : 2 sockets, 5â€“20 cores  
- **Pas de GPU**

---

### ğŸ“Š Benchmark Snake Game â€“ Prompt : `Tu peux me gÃ©nÃ©rer le jeu snake en python`

#### âœ… TEST 1 â€“ 16 Go RAM / 2 Sockets / 5 Cores

| ModÃ¨le                   | Taille   | Temps     | Ã‰tat     | CPU    | RAM    |
|--------------------------|----------|-----------|----------|--------|--------|
| `gemma3n:e4b`            | 1.5 Go   | 9 min     | âŒ       | 100 %  | 99 %   |
| `gemma3:4b`              | 3.3 Go   | 9 min     | âŒ       | 100 %  | 57 %   |
| `deepseek-r1:1.5b`       | 1.1 Go   | 6 min     | âœ…       | 100 %  | 27 %   |
| `mistral`                | 4.0 Go   | 3 min 41  | âœ…       | 100 %  | 67 %   |
| `deepseek-coder:6.7b`    | 3.8 Go   | 8 min     | âŒ       | 100 %  | 83 %   |

---

#### âœ… TEST 2 â€“ 16 Go RAM / 1 Socket / 12 Cores

| ModÃ¨le                   | Taille   | Temps     | Ã‰tat     | CPU    | RAM    |
|--------------------------|----------|-----------|----------|--------|--------|
| `gemma3:4b`              | 3.3 Go   | 8 min 35  | âŒ       | 99 %   | 52 %   |
| `deepseek-r1:1.5b`       | 1.1 Go   | 3 min 30  | âœ…       | 99 %   | 21 %   |
| `mistral`                | 4.0 Go   | 4 min     | âœ…       | 100 %  | 62 %   |
| `deepseek-coder:6.7b`    | 3.8 Go   | 6 min 18  | ğŸŸ¡ RÃ©serve | 100 %  | 74 %   |

---

#### âœ… TEST 3 â€“ 24 Go RAM / 1 Socket / 20 Cores

| ModÃ¨le                   | Taille   | Temps     | Ã‰tat     | CPU    | RAM    |
|--------------------------|----------|-----------|----------|--------|--------|
| `deepseek-r1:1.5b`       | 1.1 Go   | 2 min 55  | âœ…       | 99 %   | 15 %   |
| `// (no thinking)`       |   //     | 1 min 30  | âœ…       | 99 %   | 15 %   |
| `mistral`                | 4.0 Go   | 3 min 30  | âœ…       | 99 %   | 41 %   |

---

## ğŸ“Œ Analyse

- **Tous les modÃ¨les utilisent le CPU Ã  100 %**, ce qui est attendu en local sans GPU.
- La **RAM est utilisÃ©e de maniÃ¨re modÃ©rÃ©e**, car les modÃ¨les sont **quantisÃ©s**.
- La **taille du modÃ¨le nâ€™est pas directement liÃ©e Ã  sa rapiditÃ©**, surtout selon lâ€™optimisation.

---

## âœ… Choix final du modÃ¨le

AprÃ¨s tests :

- **`deepseek-r1:1.5b`** : modÃ¨le lÃ©ger, bon rapport vitesse/qualitÃ©, supporte le â€œthinkingâ€.
- **`mistral`** : performances stables, compatible franÃ§ais, sÃ©lectionnÃ© pour lâ€™intÃ©gration.
- **`phi3:mini`** : testÃ© plus tard, **trÃ¨s fluide**, parfait pour une interface web lÃ©gÃ¨re.

---

## ğŸ§ª Update 7 Juillet â€“ Serveur & Interface Web

- IntÃ©gration dâ€™un **serveur local avec Flask (Python)** pour combiner front et IA.
- ProblÃ¨me : la gÃ©nÃ©ration de rÃ©ponse est **plus lente via interface** que via terminal.
- Test du modÃ¨le **phi3:mini (Microsoft)** : bien plus fluide sur interface web.

### âœ… Interface Web (Prototype)

- Authentification (login, register)
- Design respectant la **charte graphique** de lâ€™entreprise
- Page **404 personnalisÃ©e**

---

## ğŸ§  Observations & Limitations

- En lâ€™absence de **GPU**, la limite optimale reste autour de **1.5B Ã  4B**.
- Plus le modÃ¨le est petit, plus il est **rÃ©actif** sur interface web.
- `deepseek-r1` avec "thinking" donne des rÃ©ponses plus dÃ©taillÃ©es mais plus lentes.
- Le modÃ¨le **phi3:mini** semble le plus adaptÃ© Ã  une interface lÃ©gÃ¨re et rÃ©active.

---

## ğŸ“ Ã€ venir / Pistes dâ€™amÃ©lioration

- Ã‰ventuellement tester avec une carte GPU externe si disponible.

---

## ğŸ“· Benchmarks Graphiques dÃ©taillÃ©s

- https://docs.google.com/document/d/12IlOcdCEi3z7ZUNSZmBwO44wdHWU5tXDkaY_lGQOwrA/edit?tab=t.rhboqz7qvf7w

---

## ğŸ› ï¸ Stack technique

- **ModÃ¨les LLM quantisÃ©s** (GGUF) : via Ollama
- **Interface Web** : HTML/CSS Tailwind + Flask
- **Backend local** : Python + serveur local (llama-server / Ollama)

---

## ğŸ“š Auteur

Projet rÃ©alisÃ© par **Tom LECLERCQ**, dans le cadre dâ€™un **stage en R&D chez Couach**, encadrÃ© par **ClÃ©ment MacadrÃ©**.

Date : **Juillet - AoÃ»t 2025**

---



