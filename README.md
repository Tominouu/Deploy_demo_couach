# CouachGPT ğŸ¤–

## ğŸ“… Contexte & Objectifs

Ce projet a pour but de **tester, comparer et intÃ©grer diffÃ©rents modÃ¨les LLM** (Large Language Models) exÃ©cutÃ©s **en local** sur serveur sans GPU. Le tout sera liÃ© sur une interface web style "ChatGPT", accessible par tous les utilisateurs connectÃ©s sur le rÃ©seau via cet **IP:** http://172.16.2.81:8294/. L'objectif est de trouver un compromis optimal entre **performances**, **consommation**, et **rÃ©activitÃ©**.

---

### âš ï¸ SI L'IA NE RÃ‰POND PLUS VOICI LA DÃ‰MARCHE Ã€ SUIVRE

- Se connecter en **ssh** Ã  la machine virtuelle:
 `ssh couachgpt@172.16.2.81` 
- Une fois dedans, tapez la commande suivante pour vÃ©rifier que le **modÃ¨le** n'a pas Ã©tÃ© **supprimÃ©**:
 `ollama ls`
- Il devrait retourner, le nom des modÃ¨les prÃ©sents.
- Si ce n'est pas le cas:
 `ollama run <nomdumodele>`
- Il va l'installer et ensuite le lancer automatiquement.
- Si le modÃ¨le est bien installÃ©, pour le redÃ©marrer faites la mÃªme commande que celle citÃ©e ci-dessus.
- Vous souhaitez arrÃªter le modÃ¨le, appuyez sur `Ctrl + D` ou tapez `/bye` dans la console.

### âš ï¸ SI LA PAGE WEB NE REPOND PLUS

- Se connecter Ã©galement en **ssh** comme il est indiquÃ© plus haut
- Se rendre dans le **dossier** couachgpt: 
 `cd couachgpt`
- Ensuite tapez:
 `python app.py`
- C'est censÃ© avoir lancÃ© le serveur, si jamais il vous dit que le port est dÃ©jÃ  pris, alors le serveur est dÃ©jÃ  lancÃ©.
- Si jamais vous Ãªtes sur que le serveur est lancÃ© et que la page n'est pas accessible, vÃ©rifiez les rÃ¨gles **NAT** du serveur, que les ports autorisÃ©s sur la **VM**:
 `sudo ufw allow 8080`


## ğŸ”¬ Phase de Recherche & Benchmarks (1â€“4 Juillet)

### ğŸ–¥ï¸ Configuration MatÃ©rielle TestÃ©e

- **RAM** : 16 Ã  24 Go  
- **CPU** : 2 sockets, 5â€“20 cores  
- **Pas de GPU**

---

### ğŸ“Š Benchmark Snake Game â€“ Prompt : `Tu peux me gÃ©nÃ©rer le jeu snake en python`

#### âœ… TEST 1 â€“ 16 Go RAM / 2 Sockets / 5 Cores

| ModÃ¨le                   | Taille   | Temps     | Ã‰tat     | CPU    | RAM    |
|--------------------------|----------|-----------|----------|--------|--------|
| `gemma3n:e4b`            | 1.5 Go   | 9 min     | âŒ       | 100 %  | 99 %   |
| `gemma3:4b`              | 3.3 Go   | 9 min     | âŒ       | 100 %  | 57 %   |
| `deepseek-r1:1.5b`       | 1.1 Go   | 6 min     | âœ…       | 100 %  | 27 %   |
| `mistral`                | 4.0 Go   | 3 min 41  | âœ…       | 100 %  | 67 %   |
| `deepseek-coder:6.7b`    | 3.8 Go   | 8 min     | âŒ       | 100 %  | 83 %   |

---

#### âœ… TEST 2 â€“ 16 Go RAM / 1 Socket / 12 Cores

| ModÃ¨le                   | Taille   | Temps     | Ã‰tat     | CPU    | RAM    |
|--------------------------|----------|-----------|----------|--------|--------|
| `gemma3:4b`              | 3.3 Go   | 8 min 35  | âŒ       | 99 %   | 52 %   |
| `deepseek-r1:1.5b`       | 1.1 Go   | 3 min 30  | âœ…       | 99 %   | 21 %   |
| `mistral`                | 4.0 Go   | 4 min     | âœ…       | 100 %  | 62 %   |
| `deepseek-coder:6.7b`    | 3.8 Go   | 6 min 18  | âŒ       | 100 %  | 74 %   |

---

#### âœ… TEST 3 â€“ 24 Go RAM / 1 Socket / 20 Cores

| ModÃ¨le                   | Taille   | Temps     | Ã‰tat     | CPU    | RAM    |
|--------------------------|----------|-----------|----------|--------|--------|
| `deepseek-r1:1.5b`       | 1.1 Go   | 2 min 55  | âœ…       | 99 %   | 15 %   |
| `// (no thinking)`       |   //     | 1 min 30  | âœ…       | 99 %   | 15 %   |
| `mistral`                | 4.0 Go   | 3 min 30  | âœ…       | 99 %   | 41 %   |

---

## ğŸ“Œ Analyse

- **Tous les modÃ¨les utilisent le CPU Ã  100 %**, ce qui est attendu en local sans GPU.
- La **RAM est utilisÃ©e de maniÃ¨re modÃ©rÃ©e**, car les modÃ¨les sont **quantisÃ©s**.
- La **taille du modÃ¨le nâ€™est pas directement liÃ©e Ã  sa rapiditÃ©**, surtout selon lâ€™optimisation.

---

## âœ… Choix final du modÃ¨le

AprÃ¨s tests :

- **`deepseek-r1:1.5b`** : modÃ¨le lÃ©ger, bon rapport vitesse/qualitÃ©, supporte le â€œthinkingâ€.
- **`mistral`** : performances stables, compatible franÃ§ais, sÃ©lectionnÃ© pour lâ€™intÃ©gration.
- **`phi3:mini`** : testÃ© plus tard, **trÃ¨s fluide**, parfait pour une interface web lÃ©gÃ¨re.

---

## ğŸ§  Observations & Limitations

- En lâ€™absence de **GPU**, la limite optimale reste autour de **1.5B Ã  4B**.
- Plus le modÃ¨le est petit, plus il est **rÃ©actif** sur interface web.
- `deepseek-r1` avec "thinking" donne des rÃ©ponses plus dÃ©taillÃ©es mais plus lentes.
- Le modÃ¨le **phi3:mini** semble le plus adaptÃ© Ã  une interface lÃ©gÃ¨re et rÃ©active.

---

## ğŸ“ Ã€ venir / Pistes dâ€™amÃ©lioration

- Ã‰ventuellement tester avec une carte GPU externe si disponible.

---

## ğŸ“· Benchmarks Graphiques dÃ©taillÃ©s

- https://docs.google.com/document/d/12IlOcdCEi3z7ZUNSZmBwO44wdHWU5tXDkaY_lGQOwrA/edit?tab=t.rhboqz7qvf7w

---

## ğŸ› ï¸ Stack technique

- **ModÃ¨les LLM quantisÃ©s** (GGUF) : via Ollama
- **Interface Web** : HTML/CSS Tailwind + Flask
- **Backend local** : Python + serveur local (llama-server / Ollama)

---

## ğŸ§ª Update 5-6 Juillet â€“ Tests des ModÃ¨les

- Tests des diffÃ©rents **modÃ¨les** retenus directement sur l'interface web **minimale**.

---

## ğŸ§ª Update 7 Juillet â€“ Serveur & Interface Web

- IntÃ©gration dâ€™un **serveur local avec Flask (Python)** pour combiner front et IA.
- ProblÃ¨me : la gÃ©nÃ©ration de rÃ©ponse est **plus lente via interface** que via terminal.
- Test du modÃ¨le **phi3:mini (Microsoft)** : bien plus fluide sur interface web.
- Authentification (login, register)
- Design respectant la **charte graphique** de lâ€™entreprise
- Page **404 personnalisÃ©e**
- DÃ©but de l'interface racine (/) (Interface de test sans style)

---


## ğŸ§ª Update 8 Juillet â€“ Serveur & Interface Web AmÃ©liorations

- AmÃ©lioration des performances d'Ollama afin de tourner correctement avec le serveur.
- L'interface **racine** a bien commencÃ©, la base **Front** est presque complÃ¨te, la partie **Back** est en cours.
- CrÃ©ation d'une base de donnÃ©e Locale Python via **Sqlite** 
- RÃ©flexion sur l'historique des conversations.

---

## ğŸ§ª Update 9 Juillet â€“ Interface + DB + Historique des conversations

- Recalibrage de la **position** des Ã©lÃ©ments sur l'interface d'interaction avec **l'IA** pour une **meilleure expÃ©rience utilisateur.**
- La **date de crÃ©ation** de la conversation est affichÃ©e sur la **page**, une configuration du **fuseau horaire** a Ã©tÃ© nÃ©cessaire afin d'avoir un rÃ©sultat **cohÃ©rent**.
- L'historique a Ã©tÃ© crÃ©Ã©, les **conversations** ainsi que leurs contenus sont enregistrÃ©s sur la base de donnÃ©e (**users.db**).
- La **suppression** des **conversations** est Ã©galement **disponible.**
- **RÃ©flexion** sur la faÃ§on de remettre en **contexte** l'IA par rapport Ã  la **discussion** (en cours)
- PossibilitÃ© de **renommer** sa **discussion**
- Ajout du **Logout** (route + bouton)
- Les **Alertes** sont maintenant disponibles sur la page de connexion (Erreurs affichÃ©es: Contraintes d'intÃ©gritÃ©)
- Ajout de la **compatibilitÃ© formatage**, lecture du **Markdown**, affichage propre du **code**, **librairies** utilisÃ©es: **Marked.js** et **Highlight.js**

---

## ğŸ§ª Update 10 Juillet â€“ Confort expÃ©rience utilisateur

- Ajout d'une popup personnalisÃ©e pour le **Delete** et le **Edit**.
- ClÃ© **ssh** active pour **commit & push**, gain de temps considÃ©rable
- DÃ©but du **responsive** pour mobile
- **Correction** du bug de la **Sidebar** qui n'Ã©tait plus active, mode **plein Ã©cran** disponible




## ğŸ“š Auteur

Projet rÃ©alisÃ© par **Tom LECLERCQ**, dans le cadre dâ€™un **stage en R&D chez Couach**, encadrÃ© par **ClÃ©ment MacadrÃ©**.

Date : **Juillet - AoÃ»t 2025**

---



